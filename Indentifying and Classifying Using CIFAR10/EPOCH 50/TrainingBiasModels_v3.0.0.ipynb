{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#* <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\"\"\"\n",
    "* CS-370 | Current/ Emerging Trends in Computer Science - 2024\n",
    "* Author: Ryan Hatch\n",
    "* Date of Development: Mon Nov 14th 06:26:26 2024 \n",
    "* Last Modified: Tues Nov 24th 12:21:21 2024\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Description: This is a simple Convolutional Neural Network model that is used to classify images from the CIFAR 10 dataset.\n",
    "The model is trained using the CIFAR 10 dataset and then evaluated on the test data. \n",
    "The model is saved to a JSON file and the weights are saved to an HDF5 file.\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "# * NOTE: Use the below code to install TensorFlow 2.0 using pip in order to get started.\n",
    "# <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\"\"\"-------------------------------------------------------------------------------------------------\n",
    "                                #* Command to use:\n",
    "pip install tensorflow==2.0.0\n",
    "pip install --upgrade tensorflow\n",
    "pip install matplotlib\n",
    "pip install numpy\n",
    "pip install keras\n",
    "pip install h5py\n",
    "pip install pillow\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "#* <><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define constants\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 50\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# Define the deeper convolutional neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional block\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu',\n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second convolutional block\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Third convolutional block\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIM,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,          # Random rotations\n",
    "    width_shift_range=0.1,      # Horizontal shifts\n",
    "    height_shift_range=0.1,     # Vertical shifts\n",
    "    horizontal_flip=True,       # Horizontal flips\n",
    "    zoom_range=0.1              # Random zoom\n",
    ")\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Train the model using data augmentation\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    epochs=NB_EPOCH,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Save model architecture to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open('cifar10_architecture.json', 'w') as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# print(\"Model architecture saved to cifar10_architecture.json\")\n",
    "\n",
    "# # Save model weights to HDF5\n",
    "# model.save_weights('cifar10_weights.h5', overwrite=True)\n",
    "# print(\"Model weights saved to cifar10_weights.h5\")\n",
    "\n",
    "# Save model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open('cifar10_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Model architecture saved to cifar10_architecture.json\")\n",
    "\n",
    "# Save model weights to HDF5 (corrected filename to end with `.weights.h5`)\n",
    "model.save_weights('cifar10_weights.weights.h5', overwrite=True)\n",
    "print(\"Model weights saved to cifar10_weights.weights.h5\")\n",
    "\n",
    "\n",
    "#* Version Two - Modified version with data augmentation and deeper network\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.datasets import cifar10\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Constants\n",
    "# IMG_CHANNELS = 3\n",
    "# IMG_ROWS = 32\n",
    "# IMG_COLS = 32\n",
    "# BATCH_SIZE = 128\n",
    "# NB_EPOCH = 20\n",
    "# NB_CLASSES = 10\n",
    "# VERBOSE = 1\n",
    "# VALIDATION_SPLIT = 0.2\n",
    "# OPTIM = RMSprop()\n",
    "\n",
    "# # Load CIFAR-10 dataset\n",
    "# (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# print('X_train shape:', X_train.shape)\n",
    "# print(X_train.shape[0], 'train samples')\n",
    "# print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# # Preprocessing\n",
    "# Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "# Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "# X_train = X_train.astype('float32') / 255\n",
    "# X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# # Data augmentation\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=15,\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.1,\n",
    "#     horizontal_flip=True)\n",
    "# datagen.fit(X_train)\n",
    "\n",
    "# # Deeper network\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(64, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(NB_CLASSES))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# # Compile and train\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "# model.fit(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "#           epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "\n",
    "# # Evaluate\n",
    "# score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "# print(\"Test score:\", score[0])\n",
    "# print(\"Test accuracy:\", score[1])\n",
    "\n",
    "#* Version One of the CNN model - Came with the book \"Deep Learning with Python\" by Francois Chollet\n",
    "\n",
    "#  from keras.datasets import cifar10\n",
    "#  from keras.utils import np_utils\n",
    "#  from keras.models import Sequential\n",
    "#  from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "#  from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "#  from keras.optimizers import SGD, Adam, RMSprop\n",
    "#  import matplotlib.pyplot as plt\n",
    "\n",
    "#  # CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "#  IMG_CHANNELS = 3\n",
    "\n",
    "#  IMG_ROWS = 32\n",
    "#  IMG_COLS = 32\n",
    "\n",
    "#  #constant\n",
    "#  BATCH_SIZE = 128\n",
    "#  NB_EPOCH = 20\n",
    "#  NB_CLASSES = 10\n",
    "#  VERBOSE = 1\n",
    "#  VALIDATION_SPLIT = 0.2\n",
    "#  OPTIM = RMSprop()\n",
    "\n",
    "#  #load dataset\n",
    "#  (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#  print('X_train shape:', X_train.shape)\n",
    "#  print(X_train.shape[0], 'train samples')\n",
    "#  print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# # convert to categorical\n",
    "#  Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "#  Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "#  # float and normalization\n",
    "#  X_train = X_train.astype('float32')\n",
    "#  X_test = X_test.astype('float32')\n",
    "#  X_train /= 255\n",
    "#  X_test /= 255\n",
    "\n",
    "# # network\n",
    "#  model = Sequential()\n",
    "#  model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#  input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "#  model.add(Activation('relu'))\n",
    "#  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#  model.add(Dropout(0.25))\n",
    "\n",
    "#  model.add(Flatten())\n",
    "#  model.add(Dense(512))\n",
    "#  model.add(Activation('relu'))\n",
    "#  model.add(Dropout(0.5))\n",
    "#  model.add(Dense(NB_CLASSES))\n",
    "#  model.add(Activation('softmax'))\n",
    "#  model.summary()\n",
    "\n",
    "#  # train\n",
    "#  model.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n",
    "#  metrics=['accuracy'])\n",
    "#  model.fit(X_train, Y_train, batch_size=BATCH_SIZE,\n",
    "#  epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,\n",
    "#  verbose=VERBOSE)\n",
    "#  score = model.evaluate(X_test, Y_test,\n",
    "#  batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "#  print(\"Test score:\", score[0])\n",
    "#  print('Test accuracy:', score[1])\n",
    "\n",
    "# #save model\n",
    "#  model_json = model.to_json()\n",
    "#  open('cifar10_architecture.json', 'w').write(model_json)\n",
    "#  And the weights learned by our deep network on the training set\n",
    "#  model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
