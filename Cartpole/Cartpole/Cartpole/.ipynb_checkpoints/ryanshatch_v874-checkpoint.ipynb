{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Five Assignment: Cartpole Problem\n",
    "Review the code in this notebook and in the score_logger.py file in the *scores* folder (directory). Once you have reviewed the code, return to this notebook and select **Cell** and then **Run All** from the menu bar to run this code. The code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installed the below packages - Setting up the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.16.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.16.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.3.1)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.16.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (41.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.4)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\" (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/61/51/046cbc61c7607e5ecead6ff1a9453fba5e7e47a5ea8d608cc7036586a5ef/scipy-1.4.1-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.27.2)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (41.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.11.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
      "Installing collected packages: scipy\n",
      "  Found existing installation: scipy 1.3.1\n",
      "    Uninstalling scipy-1.3.1:\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\scipy\\\\cluster\\\\hierarchy.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install numpy\n",
    "# %pip install gym\n",
    "# %pip install torch\n",
    "# %pip install matplotlib\n",
    "# %pip install tensorflow\n",
    "# %pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default Paramaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "  \n",
    "from scores.score_logger import ScoreLogger\n",
    "\n",
    "my_bool = bool(True)  # Define a NumPy boolean variable\n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "#* Version 1: Main code for default parameters.\n",
    "\n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "class DQNSolver:\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.model = Sequential()\n",
    "        # Define the input layer with input_shape in the first Dense layer instead of using Input\n",
    "        self.model.add(Dense(24, activation=\"relu\", input_shape=(observation_space,)))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= EXPLORATION_MAX:\n",
    "            return random.randrange(self.action_space)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + GAMMA * np.amax(self.model.predict(next_state, verbose=0)[0]))\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        global EXPLORATION_MAX\n",
    "        if EXPLORATION_MAX > EXPLORATION_MIN:\n",
    "            EXPLORATION_MAX *= EXPLORATION_DECAY\n",
    "\n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  # Initialize the environment\n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle tuple return (state, info)\n",
    "            state = state[0]\n",
    "        print(f\"Initial state: {state}, type: {type(state)}, shape: {np.array(state).shape}\")  # Debugging print statement\n",
    "        state = np.array(state, dtype=np.float32)  # Convert to NumPy array\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        print(f\"Reshaped initial state shape: {state.shape}\")  # Debugging print statement\n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, _ = env.step(action)  # Removed unused variable 'info'\n",
    "            reward = reward if not terminal else -reward  \n",
    "            print(f\"State next: {state_next}, type: {type(state_next)}, shape: {np.array(state_next).shape}\")  # Debugging print statement\n",
    "            state_next = np.array(state_next, dtype=np.float32)  # Convert to NumPy array\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            print(f\"Reshaped state next shape: {state_next.shape}\")  # Debugging print statement\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print(\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: [-0.02176946  0.00541965 -0.01961372  0.03212376], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped initial state shape: (1, 4)\n",
      "State next: [-0.02166106 -0.18941562 -0.01897124  0.31855438], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.02544938 -0.38426231 -0.01260016  0.60519461], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-3.31346225e-02 -5.79205816e-01 -4.96265050e-04  8.93882307e-01], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.04471874 -0.77432103  0.01738138  1.18640919], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.06020516 -0.57942875  0.04110956  0.89922479], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.07179373 -0.775083    0.05909406  1.20454103], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.08729539 -0.9709169   0.08318488  1.51514266], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.10671373 -0.77689436  0.11348773  1.24954337], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.12225162 -0.97327318  0.1384786   1.57551051], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.14171708 -1.16974762  0.16998881  1.90798365], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.16511204 -1.36624845  0.20814849  2.24822291], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [-0.192437   -1.17360907  0.25311294  2.02625569], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "Run: 1, exploration: 1.0, score: 12\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../scores/scores.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e23913c48dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-ecfd56bcef7c>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", exploration: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", score: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mscore_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\Module Five\\Cartpole\\Cartpole\\scores\\score_logger.py\u001b[0m in \u001b[0;36madd_score\u001b[1;34m(self, score, run)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSCORES_CSV_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         self._save_png(input_path=SCORES_CSV_PATH,  \n\u001b[0;32m    131\u001b[0m                       \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSCORES_PNG_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\Module Five\\Cartpole\\Cartpole\\scores\\score_logger.py\u001b[0m in \u001b[0;36m_save_csv\u001b[1;34m(self, path, score)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mscores_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../scores/scores.csv'"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rshat\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rshat\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rshat\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install numpy==1.23.5\n",
    "# %pip install gym\n",
    "# %pip install torch\n",
    "# %pip install matplotlib\n",
    "# %pip install tensorflow\n",
    "# %pip install keras\n",
    "\n",
    "# import random  \n",
    "# import gym  \n",
    "# import numpy as np  \n",
    "# from collections import deque  \n",
    "# from tensorflow.keras.models import Sequential  \n",
    "# from tensorflow.keras.layers import Dense, Input  # Import Input layer\n",
    "# from tensorflow.keras.optimizers import Adam  \n",
    "  \n",
    "# from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "# ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "# #* Version 1: Main code for default parameters.\n",
    "\n",
    "# GAMMA = 0.95  \n",
    "# LEARNING_RATE = 0.001  \n",
    "  \n",
    "# MEMORY_SIZE = 1000000  \n",
    "# BATCH_SIZE = 20  \n",
    "  \n",
    "# EXPLORATION_MAX = 1.0  \n",
    "# EXPLORATION_MIN = 0.01  \n",
    "# EXPLORATION_DECAY = 0.995\n",
    "\n",
    "# class DQNSolver:\n",
    "#     def __init__(self, observation_space, action_space):\n",
    "#         self.action_space = action_space\n",
    "#         self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "#         self.model = Sequential()\n",
    "#         self.model.add(Input(shape=(observation_space,)))  # Use Input layer to define input shape\n",
    "#         self.model.add(Dense(24, activation=\"relu\"))\n",
    "#         self.model.add(Dense(24, activation=\"relu\"))\n",
    "#         self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "#         self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "#     def remember(self, state, action, reward, next_state, done):\n",
    "#         self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "#     def act(self, state):\n",
    "#         if np.random.rand() <= EXPLORATION_MAX:\n",
    "#             return random.randrange(self.action_space)\n",
    "#         act_values = self.model.predict(state)\n",
    "#         return np.argmax(act_values[0])\n",
    "\n",
    "#     def experience_replay(self):\n",
    "#         if len(self.memory) < BATCH_SIZE:\n",
    "#             return\n",
    "#         batch = random.sample(self.memory, BATCH_SIZE)\n",
    "#         for state, action, reward, next_state, done in batch:\n",
    "#             target = reward\n",
    "#             if not done:\n",
    "#                 target = (reward + GAMMA * np.amax(self.model.predict(next_state)[0]))\n",
    "#             target_f = self.model.predict(state)\n",
    "#             target_f[0][action] = target\n",
    "#             self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "#         global EXPLORATION_MAX\n",
    "#         if EXPLORATION_MAX > EXPLORATION_MIN:\n",
    "#             EXPLORATION_MAX *= EXPLORATION_DECAY\n",
    "\n",
    "# def cartpole():  \n",
    "#     env = gym.make(ENV_NAME)  # Initialize the environment\n",
    "#     score_logger = ScoreLogger(ENV_NAME)  \n",
    "#     observation_space = env.observation_space.shape[0]  \n",
    "#     action_space = env.action_space.n  \n",
    "#     dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "#     run = 0  \n",
    "#     while True:  \n",
    "#         run += 1  \n",
    "#         state = env.reset()\n",
    "#         print(f\"Initial state: {state}, type: {type(state)}, shape: {np.array(state).shape}\")  # Debugging print statement\n",
    "#         state = np.array(state, dtype=np.float32)  # Convert to NumPy array\n",
    "#         state = np.reshape(state, [1, observation_space])\n",
    "#         print(f\"Reshaped initial state shape: {state.shape}\")  # Debugging print statement\n",
    "#         step = 0  \n",
    "#         while True:  \n",
    "#             step += 1  \n",
    "#             action = dqn_solver.act(state)  \n",
    "#             state_next, reward, terminal, _ = env.step(action)  # Removed unused variable 'info'\n",
    "#             reward = reward if not terminal else -reward  \n",
    "#             print(f\"State next: {state_next}, type: {type(state_next)}, shape: {np.array(state_next).shape}\")  # Debugging print statement\n",
    "#             state_next = np.array(state_next, dtype=np.float32)  # Convert to NumPy array\n",
    "#             state_next = np.reshape(state_next, [1, observation_space])\n",
    "#             print(f\"Reshaped state next shape: {state_next.shape}\")  # Debugging print statement\n",
    "#             dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "#             state = state_next  \n",
    "#             if terminal:  \n",
    "#                 print(\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "#                 score_logger.add_score(step, run)  \n",
    "#                 break  \n",
    "#             dqn_solver.experience_replay()\n",
    "\n",
    "# import random  \n",
    "# import gym  \n",
    "# import numpy as np  \n",
    "# from collections import deque  \n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "  \n",
    "  \n",
    "# from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "# ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "# GAMMA = 0.95  \n",
    "# LEARNING_RATE = 0.001  \n",
    "  \n",
    "# MEMORY_SIZE = 1000000  \n",
    "# BATCH_SIZE = 20  \n",
    "  \n",
    "# EXPLORATION_MAX = 1.0  \n",
    "# EXPLORATION_MIN = 0.01  \n",
    "# EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "# class DQNSolver:  \n",
    "  \n",
    "#     def __init__(self, observation_space, action_space):  \n",
    "#         self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "#         self.action_space = action_space  \n",
    "#         self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "#         self.model = Sequential()  \n",
    "#         self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "#         self.model.add(Dense(24, activation=\"relu\"))  \n",
    "#         self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "#         # self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "#         self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "  \n",
    "#     def remember(self, state, action, reward, next_state, done):  \n",
    "#         self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "#     def act(self, state):  \n",
    "#         if np.random.rand() < self.exploration_rate:  \n",
    "#             return random.randrange(self.action_space)  \n",
    "#         q_values = self.model.predict(state)  \n",
    "#         return np.argmax(q_values[0])  \n",
    "  \n",
    "#     def experience_replay(self):  \n",
    "#         if len(self.memory) < BATCH_SIZE:  \n",
    "#             return  \n",
    "#         batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "#         for state, action, reward, state_next, terminal in batch:  \n",
    "#             q_update = reward  \n",
    "#             if not terminal:  \n",
    "#                 q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "#             q_values = self.model.predict(state)  \n",
    "#             q_values[0][action] = q_update  \n",
    "#             self.model.fit(state, q_values, verbose=0)  \n",
    "#         self.exploration_rate *= EXPLORATION_DECAY  \n",
    "#         self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "# def cartpole():\n",
    "#     env = gym.make(ENV_NAME)\n",
    "#     score_logger = ScoreLogger(ENV_NAME)\n",
    "#     observation_space = env.observation_space.shape[0]\n",
    "#     action_space = env.action_space.n\n",
    "#     dqn_solver = DQNSolver(observation_space, action_space)\n",
    "#     run = 0\n",
    "#     while True:\n",
    "#         run += 1\n",
    "#         # Updated reset handling\n",
    "#         reset_result = env.reset()\n",
    "#         if isinstance(reset_result, tuple):  # Handle tuple return (state, info)\n",
    "#             state = reset_result[0]\n",
    "#         else:\n",
    "#             state = reset_result\n",
    "\n",
    "#         state = np.array(state, dtype=np.float32)  # Convert to NumPy array\n",
    "#         state = np.reshape(state, [1, observation_space])\n",
    "#         step = 0\n",
    "#         while True:\n",
    "#             step += 1\n",
    "#             action = dqn_solver.act(state)\n",
    "#             state_next, reward, terminal, info = env.step(action)\n",
    "#             state_next = np.array(state_next, dtype=np.float32)  # Ensure NumPy array\n",
    "#             state_next = np.reshape(state_next, [1, observation_space])\n",
    "#             reward = reward if not terminal else -reward\n",
    "#             dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "#             state = state_next\n",
    "#             if terminal:\n",
    "#                 print(f\"Run: {run}, exploration: {dqn_solver.exploration_rate}, score: {step}\")\n",
    "#                 score_logger.add_score(step, run)\n",
    "#                 break\n",
    "#             dqn_solver.experience_replay()\n",
    "\n",
    "\n",
    "# # def cartpole():  \n",
    "# #     env = gym.make(ENV_NAME)  \n",
    "# #     score_logger = ScoreLogger(ENV_NAME)  \n",
    "# #     observation_space = env.observation_space.shape[0]  \n",
    "# #     action_space = env.action_space.n  \n",
    "# #     dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "# #     run = 0  \n",
    "# #     while True:  \n",
    "# #         run += 1\n",
    "# #         state = env.reset()  \n",
    "# #         # state = np.reshape(state, [1, observation_space])\n",
    "# #         state = np.array(env.reset(), dtype=np.float32)  # Convert to NumPy array\n",
    "# #         state = np.reshape(state, [1, observation_space])\n",
    "# #         step = 0  \n",
    "# #         while True:  \n",
    "# #             step += 1  \n",
    "# #             #env.render()  \n",
    "# #             action = dqn_solver.act(state)  \n",
    "# #             state_next, reward, terminal, info = env.step(action)  \n",
    "# #             reward = reward if not terminal else -reward  \n",
    "# #             state_next = np.reshape(state_next, [1, observation_space])  \n",
    "# #             dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "# #             state = state_next  \n",
    "# #             if terminal:  \n",
    "# #                 print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "# #                 score_logger.add_score(step, run)  \n",
    "# #                 break  \n",
    "# #             dqn_solver.experience_replay()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: [ 0.00502634 -0.00977278 -0.03129624 -0.04537497], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped initial state shape: (1, 4)\n",
      "State next: [ 0.00483088  0.18578364 -0.03220374 -0.34776555], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.00854655  0.38134848 -0.03915905 -0.65042698], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.01617352  0.57699336 -0.05216759 -0.95517914], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.02771339  0.38261047 -0.07127117 -0.67933178], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.0353656   0.5786463  -0.08485781 -0.99357499], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.04693852  0.38475578 -0.10472931 -0.72870399], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.05463364  0.58115754 -0.11930339 -1.05242663], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.06625679  0.77764201 -0.14035192 -1.38005041], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.08180963  0.58452319 -0.16795293 -1.13434875], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.09350009  0.78139639 -0.1906399  -1.47464961], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "State next: [ 0.10912802  0.9782674  -0.2201329  -1.82031536], type: <class 'numpy.ndarray'>, shape: (4,)\n",
      "Reshaped state next shape: (1, 4)\n",
      "Run: 1, exploration: 1.0, score: 11\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../scores/scores.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-09fc815a5acd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m                         \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-09fc815a5acd>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", exploration: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", score: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                                 \u001b[0mscore_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                         \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\Module Five\\Cartpole\\Cartpole\\scores\\score_logger.py\u001b[0m in \u001b[0;36madd_score\u001b[1;34m(self, score, run)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSCORES_CSV_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         self._save_png(input_path=SCORES_CSV_PATH,  \n\u001b[0;32m     32\u001b[0m                       \u001b[0moutput_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSCORES_PNG_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\Module Five\\Cartpole\\Cartpole\\scores\\score_logger.py\u001b[0m in \u001b[0;36m_save_csv\u001b[1;34m(self, path, score)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mscores_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../scores/scores.csv'"
     ]
    }
   ],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Input  # Import Input layer\n",
    "from tensorflow.keras.optimizers import Adam  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "def cartpole():  \n",
    "\tenv = gym.make(ENV_NAME)  # Initialize the environment\n",
    "\tscore_logger = ScoreLogger(ENV_NAME)  \n",
    "\tobservation_space = env.observation_space.shape[0]  \n",
    "\taction_space = env.action_space.n  \n",
    "\tdqn_solver = DQNSolver(observation_space, action_space)  \n",
    "\trun = 0  \n",
    "\twhile True:  \n",
    "\t\trun += 1  \n",
    "\t\tstate = env.reset()\n",
    "\t\tprint(f\"Initial state: {state}, type: {type(state)}, shape: {np.array(state).shape}\")  # Debugging print statement\n",
    "\t\tstate = np.array(state, dtype=np.float32)  # Convert to NumPy array\n",
    "\t\tstate = np.reshape(state, [1, observation_space])\n",
    "\t\tprint(f\"Reshaped initial state shape: {state.shape}\")  # Debugging print statement\n",
    "\t\tstep = 0  \n",
    "\t\twhile True:  \n",
    "\t\t\tstep += 1  \n",
    "\t\t\taction = dqn_solver.act(state)  \n",
    "\t\t\tstate_next, reward, terminal, _ = env.step(action)  # Removed unused variable 'info'\n",
    "\t\t\treward = reward if not terminal else -reward  \n",
    "\t\t\tprint(f\"State next: {state_next}, type: {type(state_next)}, shape: {np.array(state_next).shape}\")  # Debugging print statement\n",
    "\t\t\tstate_next = np.array(state_next, dtype=np.float32)  # Convert to NumPy array\n",
    "\t\t\tstate_next = np.reshape(state_next, [1, observation_space])\n",
    "\t\t\tprint(f\"Reshaped state next shape: {state_next.shape}\")  # Debugging print statement\n",
    "\t\t\tdqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "\t\t\tstate = state_next  \n",
    "\t\t\tif terminal:  \n",
    "\t\t\t\tprint(\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "\t\t\t\tscore_logger.add_score(step, run)  \n",
    "\t\t\t\tbreak  \n",
    "\t\t\tdqn_solver.experience_replay()\n",
    "\n",
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# * Version 1: Main code for ipython notebook but the code is not working.\n",
    "# * The code is not working because the environment is not being rendered.\n",
    "# * In order to solve this issue, the code needs to be run in a python script on the terminal.\n",
    "\n",
    "# import random  \n",
    "# import numpy as np  \n",
    "# from collections import deque  \n",
    "# from keras.models import Sequential  \n",
    "# from keras.layers import Dense, Input  \n",
    "# from keras.optimizers import Adam  \n",
    "# import os\n",
    "# import gym\n",
    "\n",
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "\n",
    "my_bool = bool(True)  # Define a NumPy boolean variable\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "#* Version 1: Main code for default parameters.\n",
    "\n",
    "# GAMMA = 0.95  \n",
    "# LEARNING_RATE = 0.001  \n",
    "  \n",
    "# MEMORY_SIZE = 1000000  \n",
    "# BATCH_SIZE = 20  \n",
    "  \n",
    "# EXPLORATION_MAX = 1.0  \n",
    "# EXPLORATION_MIN = 0.01  \n",
    "# EXPLORATION_DECAY = 0.995\n",
    "\n",
    "#* Version 2: Increased Exploration Decay\n",
    "#\n",
    "# Experiment 2: Increased Exploration Decay\n",
    "# GAMMA = 0.95\n",
    "# LEARNING_RATE = 0.001\n",
    "# EXPLORATION_MAX = 1.0\n",
    "# EXPLORATION_MIN = 0.01\n",
    "# EXPLORATION_DECAY = 0.99  # Slower decay, longer exploration phase\n",
    "# MEMORY_SIZE = 1000000\n",
    "# BATCH_SIZE = 20\n",
    "\n",
    "#* Version 3: Increased Learning Rate\n",
    "#\n",
    "# Experiment 3: Higher Learning Rate\n",
    "# GAMMA = 0.95\n",
    "# LEARNING_RATE = 0.005  # Faster learning\n",
    "# EXPLORATION_MAX = 1.0\n",
    "# EXPLORATION_MIN = 0.01\n",
    "# EXPLORATION_DECAY = 0.995\n",
    "# MEMORY_SIZE = 1000000\n",
    "# BATCH_SIZE = 20\n",
    "\n",
    "#* Version 4: Reduced Discount Factor\n",
    "#\n",
    "# Experiment 4: Reduced Discount Factor\n",
    "# GAMMA = 0.8  # Less emphasis on future rewards\n",
    "# LEARNING_RATE = 0.001\n",
    "# EXPLORATION_MAX = 1.0\n",
    "# EXPLORATION_MIN = 0.01\n",
    "# EXPLORATION_DECAY = 0.995\n",
    "# MEMORY_SIZE = 1000000\n",
    "# BATCH_SIZE = 20\n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "  \n",
    "# def cartpole():  \n",
    "#     env = gym.make(ENV_NAME)  \n",
    "#     score_logger = ScoreLogger(ENV_NAME)  \n",
    "#     observation_space = env.observation_space.shape[0]  \n",
    "#     action_space = env.action_space.n  \n",
    "#     dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "#     run = 0  \n",
    "#     while True:  \n",
    "#         run += 1  \n",
    "#         state = np.array(env.reset(), dtype=np.float32)  # Convert to NumPy array\n",
    "#         state = np.reshape(state, [1, observation_space])\n",
    "#         # state = env.reset()     \n",
    "#         # state = np.reshape(state, [1, observation_space])  \n",
    "#         step = 0  \n",
    "#         while True:  \n",
    "#             step += 1  \n",
    "#             #env.render()  \n",
    "#             action = dqn_solver.act(state)  \n",
    "#             state_next, reward, terminal, info = env.step(action)  \n",
    "#             reward = reward if not terminal else -reward  \n",
    "#             # state_next = np.reshape(state_next, [1, observation_space])\n",
    "#             state_next = np.array(state_next, dtype=np.float32)  # Convert to NumPy array\n",
    "#             state_next = np.reshape(state_next, [1, observation_space])  \n",
    "#             dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "#             state = state_next  \n",
    "#             if terminal:  \n",
    "#                 print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "#                 score_logger.add_score(step, run)  \n",
    "#                 break  \n",
    "#             dqn_solver.experience_replay()\n",
    "\n",
    "# def cartpole():  \n",
    "#     env = gym.make(ENV_NAME)  # Initialize the environment\n",
    "#     score_logger = ScoreLogger(ENV_NAME)  \n",
    "#     observation_space = env.observation_space.shape[0]  \n",
    "#     action_space = env.action_space.n  \n",
    "#     dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "#     run = 0  \n",
    "#     while True:  \n",
    "#         run += 1  \n",
    "#         state = np.array(env.reset(), dtype=np.float32)  # Convert to NumPy array\n",
    "#         state = np.reshape(state, [1, observation_space])\n",
    "#         step = 0  \n",
    "#         while True:  \n",
    "#             state_next, reward, terminal, _ = env.step(action)  \n",
    "#             reward = reward if not terminal else -1  \n",
    "#             state_next, reward, terminal, info = env.step(action)  \n",
    "#             reward = reward if not terminal else -reward  \n",
    "#             state_next = np.array(state_next, dtype=np.float32)  # Convert to NumPy array\n",
    "#             state_next = np.reshape(state_next, [1, observation_space])  \n",
    "#             dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "#             state = state_next  \n",
    "#             if terminal:  \n",
    "#                 print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "#                 score_logger.add_score(step, run)  \n",
    "#                 break  \n",
    "#             dqn_solver.experience_replay()\n",
    "\n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  # Initialize the environment\n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Ensure state is a NumPy array\n",
    "            state = np.concatenate(state)\n",
    "        print(f\"Initial state: {state}, shape: {np.array(state).shape}\")  # Debugging print statement\n",
    "        state = np.array(state, dtype=np.float32)  # Convert to NumPy array\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        print(f\"Reshaped initial state shape: {state.shape}\")  # Debugging print statement\n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            if isinstance(state_next, tuple):  # Ensure state_next is a NumPy array\n",
    "                state_next = np.concatenate(state_next)\n",
    "            reward = reward if not terminal else -reward  \n",
    "            print(f\"State next: {state_next}, shape: {np.array(state_next).shape}\")  # Debugging print statement\n",
    "            state_next = np.array(state_next, dtype=np.float32)  # Convert to NumPy array\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            print(f\"Reshaped state next shape: {state_next.shape}\")  # Debugging print statement\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print(\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EXPLORATION_MAX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e23913c48dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-06aa72aa7d77>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m()\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mobservation_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0mdqn_solver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNSolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-06aa72aa7d77>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, observation_space, action_space)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEXPLORATION_MAX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EXPLORATION_MAX' is not defined"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the code is running properly, you should begin to see output appearing above this code block. It will take several minutes, so it is recommended that you let this code run in the background while completing other work. When the code has finished, it will print output saying, \"Solved in _ runs, _ total runs.\"\n",
    "\n",
    "You may see an error about not having an exit command. This error does not affect the program's functionality and results from the steps taken to convert the code from Python 2.x to Python 3. Please disregard this error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
